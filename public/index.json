[{
    "title": "Avatar: The Last Airbender",
    "date": "",
    "description": "A TidyTuesday project modelling Avatar TV show scripts.",
    "body": " Today we are looking at text data from the scripts for the TV show ‘Avatar: The Last Airbender’.\nI decided to see if certain characters that dominate a show, based on their word count, affects the ratings in a postive or negative way. It would be interesting to incorporate screen time as well, but all I have here is word count.\nSetup First we setup our enivronment and load the dataset.\nlibrary(tidytuesdayR) library(tidyverse) ## ── Attaching packages ────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.1 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ───────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(knitr) library(kableExtra) ## ## Attaching package: \u0026#39;kableExtra\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## group_rows knitr::opts_chunk$set( echo = FALSE, fig.height = 5, fig.width = 8, message = FALSE, warning = FALSE, dpi = 180 ) Load the data\n## ## Downloading file 1 of 2: `avatar.csv` ## Downloading file 2 of 2: `scene_description.csv`  Data Manipulation and Cleanup Next we create dataframes to capture the text data for all characters, the totals per episode, then join them.\nCreate list of top 6 characters by word count. We will only plot these characters as they have the most words compared to the other characters.\nBelow is a first crack. I create a scatter plot and then use stat_ellipse to get a rough idea of the distribution and centroid for each character. Each point is an episode. I ignore which book/season of the series is involved, but it might be interesting to see how character’s evolve over the arc of the entire series.\nNow let’s make the plot pretty.\n Notice we can use the group.colors named list to dictate who gets assigned which color. Neat!\n I use a density 2D plot to help show how the points are distributed.\nI don’t think this came out quite as informative as I would have liked, but it is interesting to see that Zuko has several episodes where he is speaking the most out of all characters, and those all appear to have relatively high ratings.\n ",
    "ref": "/blog/avatar-the-last-airbender/"
  },{
    "title": "About",
    "date": "",
    "description": "Data Science :: Analytics :: Visualization",
    "body": "Hi, I\u0026rsquo;m Joseph Pope, a data scientist/analyst/visualizer and business intelligence professional living in New York City.\nWhile I enjoy taking online classes to learn new skills and techniques, I have found that working on projects regularly really helps reinforce the lessons that I am learning. I have kept myself busy with open data #civictech type projecst in the past, Rstudio\u0026rsquo;s #TidyTuesday weekly projects, and other ad hoc work with Tableau, R and python.\nProjects My first tweet for TidyTuesday was back in January. It is interesting to watch the progress as my data anlaysis skills have improved alongside my visualization skills.\nFirst #TidyTuesday attempt to start off 2020. Trying a dumbell plot (ggalt package) with min and max temps for Australia. #r4ds #rstats Code here: https://t.co/HcIu0NtY1k pic.twitter.com/G5MbQNPvfW\n\u0026mdash; Joseph W Pope (@joepope44) January 9, 2020  #TidyTuesday 2020 Portfolio\nR Script to Scrape NBA Playoff Data\nOther Github Repos\n",
    "ref": "/about/"
  },{
    "title": "Coffee Ratings",
    "date": "",
    "description": "TidyTuesday project modelling coffee ratings",
    "body": "\nIntro Today I am going to explore Coffee Ratings for this week’s Tidy Tuesday challenge. The data comes from the Coffee Quality Database. The great thing about Tidy Tuesday is that we can do whatever we want with the data. There are no rules or directions!\nThis data is very interesting with many features and looks ripe for a predictive model. I can use the various ratings provided to predict either a quantitative method, like what the ‘total_cup_score’ (Overall rating) is, or a qualitative measure, like what species of bean (Arabica or Robusta) or perhaps the country of origin.\nPredicting scores strikes me as a classic regression problem, best tackled by linear models or random forests, and classification could be solved with logistic regression, Naive Bayes, SVM or even neural networks. I’ve NEVER used tidymodels before, so let’s give it a go, make some mistakes, and see what we can learn.\nBut first, some brief background of our data from the TidyTuesday repo:\n There is data for both Arabica and Robusta beans, across many countries and professionally rated on a 0-100 scale. All sorts of scoring/ratings for things like acidity, sweetness, fragrance, balance, etc - may be useful for either separating into visualizations/categories or for modeling/recommenders.\n  Wikipedia on Coffee Beans:\n  The two most economically important varieties of coffee plant are the Arabica and the Robusta; ~60% of the coffee produced worldwide is Arabica and ~40% is Robusta. Arabica beans consist of 0.8–1.4% caffeine and Robusta beans consist of 1.7–4% caffeine.\n Setup\nlibrary(tidytuesdayR) library(tidyverse) ## ── Attaching packages ──────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.1 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0 ## ── Conflicts ─────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tidymodels) ## ── Attaching packages ─────────────────── tidymodels 0.1.0 ── ## ✓ broom 0.5.6 ✓ rsample 0.0.6 ## ✓ dials 0.0.6 ✓ tune 0.1.0 ## ✓ infer 0.5.1 ✓ workflows 0.1.1 ## ✓ parsnip 0.1.1 ✓ yardstick 0.0.6 ## ✓ recipes 0.1.12 ## ── Conflicts ────────────────────── tidymodels_conflicts() ── ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x dials::margin() masks ggplot2::margin() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() library(knitr) library(kableExtra) ## ## Attaching package: \u0026#39;kableExtra\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## group_rows knitr::opts_chunk$set( fig.height = 5, fig.width = 8, message = FALSE, warning = FALSE, dpi = 180, include = TRUE ) Load the data\ntuesdata \u0026lt;- tidytuesdayR::tt_load(2020, week = 28) coffee_ratings \u0026lt;- tuesdata$coffee_ratings  EDA Now that we have our data and let’s dive in. Let’s review the summary statistics with skimr.\ncoffee_ratings %\u0026gt;% skimr::skim()  Table 1: Data summary          Name  Piped data    Number of rows  1339    Number of columns  43    _______________________     Column type frequency:     character  24    numeric  19    ________________________     Group variables  None     Variable type: character\n   skim_variable  n_missing  complete_rate  min  max  empty  n_unique  whitespace      species  0  1.00  7  7  0  2  0    owner  7  0.99  3  50  0  315  0    country_of_origin  1  1.00  4  28  0  36  0    farm_name  359  0.73  1  73  0  571  0    lot_number  1063  0.21  1  71  0  227  0    mill  315  0.76  1  77  0  460  0    ico_number  151  0.89  1  40  0  847  0    company  209  0.84  3  73  0  281  0    altitude  226  0.83  1  41  0  396  0    region  59  0.96  2  76  0  356  0    producer  231  0.83  1  100  0  691  0    bag_weight  0  1.00  1  8  0  56  0    in_country_partner  0  1.00  7  85  0  27  0    harvest_year  47  0.96  3  24  0  46  0    grading_date  0  1.00  13  20  0  567  0    owner_1  7  0.99  3  50  0  319  0    variety  226  0.83  4  21  0  29  0    processing_method  170  0.87  5  25  0  5  0    color  218  0.84  4  12  0  4  0    expiration  0  1.00  13  20  0  566  0    certification_body  0  1.00  7  85  0  26  0    certification_address  0  1.00  40  40  0  32  0    certification_contact  0  1.00  40  40  0  29  0    unit_of_measurement  0  1.00  1  2  0  2  0     Variable type: numeric\n   skim_variable  n_missing  complete_rate  mean  sd  p0  p25  p50  p75  p100  hist      total_cup_points  0  1.00  82.09  3.50  0  81.08  82.50  83.67  90.58  ▁▁▁▁▇    number_of_bags  0  1.00  154.18  129.99  0  14.00  175.00  275.00  1062.00  ▇▇▁▁▁    aroma  0  1.00  7.57  0.38  0  7.42  7.58  7.75  8.75  ▁▁▁▁▇    flavor  0  1.00  7.52  0.40  0  7.33  7.58  7.75  8.83  ▁▁▁▁▇    aftertaste  0  1.00  7.40  0.40  0  7.25  7.42  7.58  8.67  ▁▁▁▁▇    acidity  0  1.00  7.54  0.38  0  7.33  7.58  7.75  8.75  ▁▁▁▁▇    body  0  1.00  7.52  0.37  0  7.33  7.50  7.67  8.58  ▁▁▁▁▇    balance  0  1.00  7.52  0.41  0  7.33  7.50  7.75  8.75  ▁▁▁▁▇    uniformity  0  1.00  9.83  0.55  0  10.00  10.00  10.00  10.00  ▁▁▁▁▇    clean_cup  0  1.00  9.84  0.76  0  10.00  10.00  10.00  10.00  ▁▁▁▁▇    sweetness  0  1.00  9.86  0.62  0  10.00  10.00  10.00  10.00  ▁▁▁▁▇    cupper_points  0  1.00  7.50  0.47  0  7.25  7.50  7.75  10.00  ▁▁▁▇▁    moisture  0  1.00  0.09  0.05  0  0.09  0.11  0.12  0.28  ▃▇▅▁▁    category_one_defects  0  1.00  0.48  2.55  0  0.00  0.00  0.00  63.00  ▇▁▁▁▁    quakers  1  1.00  0.17  0.83  0  0.00  0.00  0.00  11.00  ▇▁▁▁▁    category_two_defects  0  1.00  3.56  5.31  0  0.00  2.00  4.00  55.00  ▇▁▁▁▁    altitude_low_meters  230  0.83  1750.71  8669.44  1  1100.00  1310.64  1600.00  190164.00  ▇▁▁▁▁    altitude_high_meters  230  0.83  1799.35  8668.81  1  1100.00  1350.00  1650.00  190164.00  ▇▁▁▁▁    altitude_mean_meters  230  0.83  1775.03  8668.63  1  1100.00  1310.64  1600.00  190164.00  ▇▁▁▁▁      Check Initial Assumptions I originally wanted to predict “total_cup_points”. However, after some review it appears that this field is simply the sum of ten other ratings, from aroma to cupper_points. What I really want to predict is Cupper Points. This is the reviewer’s (aka the Cupper’s) personal overall rating for the cup of joe. I can use this as dependant variable in a regression analysis. This should help me identify which features (aroma, acidity, elevation, country of origin?) influence the final rating the most.\n Potential Features Before we dig further into the data, let’s speculate what might factors go into a superior coffee bean. If coffee is like wine, then perhaps region (lat, long, altitude, preciptation, average hours of sunlight, etc) makes a big difference. Perhaps certain companies utilize best practices and farming techniques. Maybe certain processing methods are better than others. We are given the testing date and the expiration date, so we can add a feature that measures how fresh the beans are. These features and more could be really important or totally useless. I have no idea! Let’s see what the data has to say about it.\nThe wiki in the readme suggests that coffee beans are split 60/40 worldwide, by Arabica and Robusta. However, for this analysis the split is much more imbalanced. I thought it would be very interesting to build an Arabica vs. Robusta classifier but with such an imbalanced set I don’t think upsampling or other methods will be enough to build a robust model.\ntable(coffee_ratings$species) ## ## Arabica Robusta ## 1311 28 Let’s take a peak at the distribution of country of origin field. It is mentioned in the articles that Ethiopia wins top prize, however most of the coffees are from Central and South America.\ncoffee_ratings %\u0026gt;% group_by(country_of_origin) %\u0026gt;% tally() %\u0026gt;% top_n(., 10) %\u0026gt;% arrange(desc(n)) ## # A tibble: 10 x 2 ## country_of_origin n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Mexico 236 ## 2 Colombia 183 ## 3 Guatemala 181 ## 4 Brazil 132 ## 5 Taiwan 75 ## 6 United States (Hawaii) 73 ## 7 Honduras 53 ## 8 Costa Rica 51 ## 9 Ethiopia 44 ## 10 Tanzania, United Republic Of 40 Let’s pull this list of top 10 countries into a list to reference later.\ntop_countries \u0026lt;- coffee_ratings %\u0026gt;% group_by(country_of_origin) %\u0026gt;% tally() %\u0026gt;% top_n(., 10) %\u0026gt;% arrange(desc(n)) %\u0026gt;% pull(country_of_origin) Out of curiousity, let’s see the count of coffee bean color. This was news to me, but apparently coffee beans are mostly blue or green in color when fresh. It is only after roasting that the color darkens to that familiar, uh, “coffee-brown” color. If you learn nothing else today, you can at least learn this.\nThe Coffee Bean Spectrum\n table(coffee_ratings$color) ## ## Blue-Green Bluish-Green Green None ## 85 114 870 52 Robusta is seen below with a higher modal peak, suggesting the species, known for its higher caffeine levels, is rated higher than Arabica. This could be due to a small sample size though.\ncoffee_ratings %\u0026gt;% mutate(id = row_number()) %\u0026gt;% select(id, species, cupper_points) %\u0026gt;% ggplot(aes(cupper_points, fill = species)) + geom_density(alpha = 0.1)  # geom_histogram(position = \u0026quot;identity\u0026quot;, alpha = 0.7, show.legend = FALSE)  TIDYMODELS I have never used tidymodels, so let’s see how this goes. Let’s try to tackle the regression model first. From the skim package, we know we have no missing values or junky data for this. Nice!\nFirst thing to do is narrow our data and make it ripe for modeling. TidyTuesday mentions that Yorgos Askalidis already determined that altitude, processing_method and color have no bearing to the cupping score, and I think that makes sense. I will omit those features for now. That leaves us with species, country of origin and several ratings and measurements of the coffee samples. I am going to take the top ten countries only (out of 36 in the data) and lump the rest into “Other” with forcats. This is simply to speed up the processing without losing too much information gain for the feature.\n Note - I went back after trying the models a few times and removed ‘species’. In this smaller, filtered dataset there are only 3 records with Robusta coffee beans. It caused me some issues, and with only 3 members of the minority class, I decided to just remove it completely.\n coffee \u0026lt;- coffee_ratings %\u0026gt;% mutate(id = row_number()) %\u0026gt;% # add ID field, convert NA to Other select(id, country_of_origin, variety, aroma:moisture) %\u0026gt;% # select features mutate(country_of_origin = case_when( country_of_origin %in% top_countries ~ country_of_origin, TRUE ~ \u0026quot;Other\u0026quot; )) %\u0026gt;% mutate_if(is.character, as.factor) %\u0026gt;% # convert country and variety to factors filter(cupper_points != 0) %\u0026gt;% # remove 0 point scores as junk data drop_na() # drop NA for country and variety fields. only affects 200 records Split the data\n Note - I initially tried to stratify by species, even though there aren’t many Robusta coffee samples in the data. I ended up removing this due to the severe class imbalance. It may have been worth the effort to upsample Robusta and then stratify by species.\n set.seed(1234) coffee_split \u0026lt;- coffee %\u0026gt;% initial_split() coffee_train \u0026lt;- training(coffee_split) coffee_test \u0026lt;- testing(coffee_split) Naive linear model with no scaling or pre-processing\nlm_spec \u0026lt;- linear_reg() %\u0026gt;% set_engine(engine = \u0026quot;lm\u0026quot;) lm_spec ## Linear Regression Model Specification (regression) ## ## Computational engine: lm ## Linear Regression Model Specification (regression) ## ## Computational engine: lm lm_fit \u0026lt;- lm_spec %\u0026gt;% fit(cupper_points ~ ., data = coffee_train ) lm_fit ## parsnip model object ## ## Fit time: 10ms ## ## Call: ## stats::lm(formula = formula, data = data) ## ## Coefficients: ## (Intercept) ## 2.501509 ## id ## -0.000309 ## country_of_originColombia ## -0.099026 ## country_of_originCosta Rica ## -0.020488 ## country_of_originEthiopia ## 0.029321 ## country_of_originGuatemala ## -0.045107 ## country_of_originHonduras ## -0.013921 ## country_of_originMexico ## 0.030019 ## country_of_originOther ## 0.003251 ## country_of_originTaiwan ## -0.134839 ## country_of_originTanzania, United Republic Of ## 0.052325 ## country_of_originUnited States (Hawaii) ## -0.316031 ## varietyBlue Mountain ## -0.234205 ## varietyBourbon ## -0.004215 ## varietyCatimor ## 0.018467 ## varietyCatuai ## 0.015110 ## varietyCaturra ## 0.036397 ## varietyEthiopian Heirlooms ## -0.094942 ## varietyEthiopian Yirgacheffe ## 0.011110 ## varietyGesha ## 0.046588 ## varietyHawaiian Kona ## 0.296594 ## varietyJava ## 0.007740 ## varietyMandheling ## 0.001044 ## varietyMarigojipe ## -0.006565 ## varietyMundo Novo ## -0.047911 ## varietyOther ## -0.019686 ## varietyPacamara ## -0.067598 ## varietyPacas ## -0.001151 ## varietyPache Comun ## -0.134161 ## varietyPeaberry ## -0.022037 ## varietyRuiru 11 ## -0.016437 ## varietySL14 ## 0.017365 ## varietySL28 ## -0.256004 ## varietySL34 ## -0.055397 ## varietySulawesi ## 0.040376 ## varietySumatra ## 0.096590 ## varietySumatra Lintong ## 0.080224 ## varietyTypica ## -0.035801 ## varietyYellow Bourbon ## 0.086077 ## aroma ## -0.003595 ## flavor ## 0.241316 ## aftertaste ## 0.278652 ## acidity ## 0.040398 ## body ## 0.022329 ## balance ## 0.194880 ## uniformity ## -0.022991 ## clean_cup ## 0.013229 ## sweetness ## -0.043329 ## moisture ## -0.505604 library(ranger) rf_spec \u0026lt;- rand_forest(mode = \u0026quot;regression\u0026quot;) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) rf_spec ## Random Forest Model Specification (regression) ## ## Computational engine: ranger ## Random Forest Model Specification (regression) ## ## Computational engine: ranger rf_fit \u0026lt;- rf_spec %\u0026gt;% fit(cupper_points ~ ., data = coffee_train ) rf_fit ## parsnip model object ## ## Fit time: 468ms ## Ranger result ## ## Call: ## ranger::ranger(formula = formula, data = data, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 834 ## Number of independent variables: 13 ## Mtry: 3 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 0.04135314 ## R squared (OOB): 0.7374583 Both models produce a hefty R squared, \u0026gt; .7! This makes me nervous as it’s likely too good to be true (overfit). I know that I skipped some preprocessing steps just to see how these models worked, so let’s go back and see if some recipes can bring my models back down to earth.\n Preprocessing With Recipes Let’s create a bland, vanilla recipe to start off  rec_obj \u0026lt;- recipe(cupper_points ~ ., data = coffee_train) %\u0026gt;% update_role(id, new_role = \u0026quot;ID\u0026quot;) %\u0026gt;% step_normalize(all_predictors(), -all_nominal()) %\u0026gt;% step_dummy(all_nominal()) summary(rec_obj) ## # A tibble: 14 x 4 ## variable type role source ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 id numeric ID original ## 2 country_of_origin nominal predictor original ## 3 variety nominal predictor original ## 4 aroma numeric predictor original ## 5 flavor numeric predictor original ## 6 aftertaste numeric predictor original ## 7 acidity numeric predictor original ## 8 body numeric predictor original ## 9 balance numeric predictor original ## 10 uniformity numeric predictor original ## 11 clean_cup numeric predictor original ## 12 sweetness numeric predictor original ## 13 moisture numeric predictor original ## 14 cupper_points numeric outcome original I tell the recipe not to use the “id” column in the analysis and we can see that it is classified as a “ID” and not “predictor”. I run summary on the recipe object and can see the type of role the variables have, as well as the type. It is important to note that ‘nominal’ is yet another term to describe a factor/string/non-numeric.\nThen I use the step_* functions to normalize the data (it centers and scales numerical features) and to create dummy variables for all of my nominal (non-numeric) features.\nSo we built our recipe. Next step is to prep and juice. I’m getting thirsty.\ncoff_train \u0026lt;- juice(prep(rec_obj)) dim(coff_train) ## [1] 834 50 names(coff_train) ## [1] \u0026quot;id\u0026quot; ## [2] \u0026quot;aroma\u0026quot; ## [3] \u0026quot;flavor\u0026quot; ## [4] \u0026quot;aftertaste\u0026quot; ## [5] \u0026quot;acidity\u0026quot; ## [6] \u0026quot;body\u0026quot; ## [7] \u0026quot;balance\u0026quot; ## [8] \u0026quot;uniformity\u0026quot; ## [9] \u0026quot;clean_cup\u0026quot; ## [10] \u0026quot;sweetness\u0026quot; ## [11] \u0026quot;moisture\u0026quot; ## [12] \u0026quot;cupper_points\u0026quot; ## [13] \u0026quot;country_of_origin_Colombia\u0026quot; ## [14] \u0026quot;country_of_origin_Costa.Rica\u0026quot; ## [15] \u0026quot;country_of_origin_Ethiopia\u0026quot; ## [16] \u0026quot;country_of_origin_Guatemala\u0026quot; ## [17] \u0026quot;country_of_origin_Honduras\u0026quot; ## [18] \u0026quot;country_of_origin_Mexico\u0026quot; ## [19] \u0026quot;country_of_origin_Other\u0026quot; ## [20] \u0026quot;country_of_origin_Taiwan\u0026quot; ## [21] \u0026quot;country_of_origin_Tanzania..United.Republic.Of\u0026quot; ## [22] \u0026quot;country_of_origin_United.States..Hawaii.\u0026quot; ## [23] \u0026quot;variety_Blue.Mountain\u0026quot; ## [24] \u0026quot;variety_Bourbon\u0026quot; ## [25] \u0026quot;variety_Catimor\u0026quot; ## [26] \u0026quot;variety_Catuai\u0026quot; ## [27] \u0026quot;variety_Caturra\u0026quot; ## [28] \u0026quot;variety_Ethiopian.Heirlooms\u0026quot; ## [29] \u0026quot;variety_Ethiopian.Yirgacheffe\u0026quot; ## [30] \u0026quot;variety_Gesha\u0026quot; ## [31] \u0026quot;variety_Hawaiian.Kona\u0026quot; ## [32] \u0026quot;variety_Java\u0026quot; ## [33] \u0026quot;variety_Mandheling\u0026quot; ## [34] \u0026quot;variety_Marigojipe\u0026quot; ## [35] \u0026quot;variety_Moka.Peaberry\u0026quot; ## [36] \u0026quot;variety_Mundo.Novo\u0026quot; ## [37] \u0026quot;variety_Other\u0026quot; ## [38] \u0026quot;variety_Pacamara\u0026quot; ## [39] \u0026quot;variety_Pacas\u0026quot; ## [40] \u0026quot;variety_Pache.Comun\u0026quot; ## [41] \u0026quot;variety_Peaberry\u0026quot; ## [42] \u0026quot;variety_Ruiru.11\u0026quot; ## [43] \u0026quot;variety_SL14\u0026quot; ## [44] \u0026quot;variety_SL28\u0026quot; ## [45] \u0026quot;variety_SL34\u0026quot; ## [46] \u0026quot;variety_Sulawesi\u0026quot; ## [47] \u0026quot;variety_Sumatra\u0026quot; ## [48] \u0026quot;variety_Sumatra.Lintong\u0026quot; ## [49] \u0026quot;variety_Typica\u0026quot; ## [50] \u0026quot;variety_Yellow.Bourbon\u0026quot; coff_train ## # A tibble: 834 x 50 ## id aroma flavor aftertaste acidity body balance uniformity clean_cup ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 3.84 3.55 3.27 3.35 3.35 2.68 0.312 0.225 ## 2 3 2.77 3.03 3.04 2.84 3.02 2.68 0.312 0.225 ## 3 5 2.22 3.03 2.54 3.10 3.35 2.41 0.312 0.225 ## 4 10 1.66 3.27 3.27 3.10 0.575 2.68 0.312 0.225 ## 5 12 2.22 2.78 2.30 2.56 2.09 1.94 0.312 0.225 ## 6 13 1.66 3.55 2.77 2.84 1.80 1.68 0.312 0.225 ## 7 16 1.40 3.03 3.51 2.05 2.42 1.44 0.312 0.225 ## 8 19 2.77 2.26 2.03 2.05 1.50 1.44 0.312 0.225 ## 9 21 1.40 2.26 2.03 3.10 2.72 1.44 0.312 0.225 ## 10 23 1.96 2.26 2.30 1.51 1.17 1.94 0.312 0.225 ## # … with 824 more rows, and 41 more variables: sweetness \u0026lt;dbl\u0026gt;, moisture \u0026lt;dbl\u0026gt;, ## # cupper_points \u0026lt;dbl\u0026gt;, country_of_origin_Colombia \u0026lt;dbl\u0026gt;, ## # country_of_origin_Costa.Rica \u0026lt;dbl\u0026gt;, country_of_origin_Ethiopia \u0026lt;dbl\u0026gt;, ## # country_of_origin_Guatemala \u0026lt;dbl\u0026gt;, country_of_origin_Honduras \u0026lt;dbl\u0026gt;, ## # country_of_origin_Mexico \u0026lt;dbl\u0026gt;, country_of_origin_Other \u0026lt;dbl\u0026gt;, ## # country_of_origin_Taiwan \u0026lt;dbl\u0026gt;, ## # country_of_origin_Tanzania..United.Republic.Of \u0026lt;dbl\u0026gt;, ## # country_of_origin_United.States..Hawaii. \u0026lt;dbl\u0026gt;, ## # variety_Blue.Mountain \u0026lt;dbl\u0026gt;, variety_Bourbon \u0026lt;dbl\u0026gt;, variety_Catimor \u0026lt;dbl\u0026gt;, ## # variety_Catuai \u0026lt;dbl\u0026gt;, variety_Caturra \u0026lt;dbl\u0026gt;, ## # variety_Ethiopian.Heirlooms \u0026lt;dbl\u0026gt;, variety_Ethiopian.Yirgacheffe \u0026lt;dbl\u0026gt;, ## # variety_Gesha \u0026lt;dbl\u0026gt;, variety_Hawaiian.Kona \u0026lt;dbl\u0026gt;, variety_Java \u0026lt;dbl\u0026gt;, ## # variety_Mandheling \u0026lt;dbl\u0026gt;, variety_Marigojipe \u0026lt;dbl\u0026gt;, ## # variety_Moka.Peaberry \u0026lt;dbl\u0026gt;, variety_Mundo.Novo \u0026lt;dbl\u0026gt;, variety_Other \u0026lt;dbl\u0026gt;, ## # variety_Pacamara \u0026lt;dbl\u0026gt;, variety_Pacas \u0026lt;dbl\u0026gt;, variety_Pache.Comun \u0026lt;dbl\u0026gt;, ## # variety_Peaberry \u0026lt;dbl\u0026gt;, variety_Ruiru.11 \u0026lt;dbl\u0026gt;, variety_SL14 \u0026lt;dbl\u0026gt;, ## # variety_SL28 \u0026lt;dbl\u0026gt;, variety_SL34 \u0026lt;dbl\u0026gt;, variety_Sulawesi \u0026lt;dbl\u0026gt;, ## # variety_Sumatra \u0026lt;dbl\u0026gt;, variety_Sumatra.Lintong \u0026lt;dbl\u0026gt;, variety_Typica \u0026lt;dbl\u0026gt;, ## # variety_Yellow.Bourbon \u0026lt;dbl\u0026gt; Baking applies the pre-processing steps that were ‘juiced’ above to the test data. This way, the train and test data will have the same pre-processing steps (aka recipe) applied to them. Clever!\ncoff_test \u0026lt;- rec_obj %\u0026gt;% prep() %\u0026gt;% bake(coffee_test)  So now we see all our predictors including all the dummy variables that were generated. Neat.\n Better, Faster, Stronger Now we can go back to the linear and random forest regression models used earlier, but now substitute the data with better processing. I think I see why they went with the “recipe” motif! Like baking, I can swap in ingredients without having to start over from scratch.\nlm_fit1 \u0026lt;- fit(lm_spec, cupper_points ~ ., coff_train) glance(lm_fit1$fit) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.751 0.736 0.204 49.4 2.55e-203 49 168. -237. -0.335 ## # … with 2 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt; tidy(lm_fit1) %\u0026gt;% arrange(desc(abs(statistic))) ## # A tibble: 49 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7.71 0.109 70.7 0. ## 2 id -0.000309 0.0000479 -6.45 1.99e-10 ## 3 aftertaste 0.0944 0.0163 5.79 1.01e- 8 ## 4 balance 0.0665 0.0132 5.02 6.39e- 7 ## 5 flavor 0.0789 0.0173 4.56 6.07e- 6 ## 6 country_of_origin_Taiwan -0.135 0.0462 -2.92 3.63e- 3 ## 7 moisture -0.0230 0.00823 -2.79 5.36e- 3 ## 8 country_of_origin_Colombia -0.0990 0.0434 -2.28 2.28e- 2 ## 9 sweetness -0.0177 0.00819 -2.16 3.09e- 2 ## 10 variety_SL28 -0.256 0.119 -2.16 3.14e- 2 ## # … with 39 more rows lm_predicted \u0026lt;- augment(lm_fit1$fit, data = coff_train) select(lm_predicted, id, cupper_points, .fitted:.std.resid) ## # A tibble: 834 x 9 ## id cupper_points .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 8.58 8.52 0.0749 0.0562 0.135 0.204 2.80e-4 0.297 ## 2 3 9.25 8.45 0.0431 0.795 0.0447 0.202 1.52e-2 3.99 ## 3 5 8.58 8.39 0.0707 0.187 0.120 0.204 2.67e-3 0.977 ## 4 10 8.5 8.49 0.0721 0.00789 0.125 0.204 5.00e-6 0.0414 ## 5 12 8.5 8.34 0.0379 0.162 0.0345 0.204 4.75e-4 0.807 ## 6 13 8.33 8.43 0.0435 -0.0979 0.0456 0.204 2.36e-4 -0.492 ## 7 16 8.17 8.43 0.0443 -0.264 0.0472 0.204 1.78e-3 -1.33 ## 8 19 8.42 8.21 0.0570 0.206 0.0783 0.204 1.92e-3 1.05 ## 9 21 8.17 8.25 0.0407 -0.0779 0.0398 0.204 1.29e-4 -0.390 ## 10 23 8.58 8.24 0.0402 0.344 0.0388 0.204 2.44e-3 1.72 ## # … with 824 more rows ggplot(lm_predicted, aes(.fitted, cupper_points)) + geom_point(alpha = .2) + ggrepel::geom_label_repel(aes(label = id), data = filter(lm_predicted, abs(.resid) \u0026gt; 2)) + labs(title = \u0026quot;Linear Model: Actual vs. Predicted Cupper Points\u0026quot;) + geom_smooth() Whoa, what’s up with coffee sample 963? We predicted a score of 7.4 or so, and the real score was a 5.25. This appears just to be a major outlier, so we can likely ignore.\nfilter(coffee, id == 963) ## # A tibble: 1 x 14 ## id country_of_orig… variety aroma flavor aftertaste acidity body balance ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 963 Taiwan Typica 7.83 7.75 7.67 7.83 7.83 7.75 ## # … with 5 more variables: uniformity \u0026lt;dbl\u0026gt;, clean_cup \u0026lt;dbl\u0026gt;, sweetness \u0026lt;dbl\u0026gt;, ## # cupper_points \u0026lt;dbl\u0026gt;, moisture \u0026lt;dbl\u0026gt; Now let’s repeat for the random forest model. Note - glance does not work for ranger objects like rf_fit1$fit.\nrf_fit1 \u0026lt;- fit(rf_spec, cupper_points ~ ., coff_train) rf_fit1$fit ## Ranger result ## ## Call: ## ranger::ranger(formula = formula, data = data, num.threads = 1, verbose = FALSE, seed = sample.int(10^5, 1)) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 834 ## Number of independent variables: 49 ## Mtry: 7 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 0.04222097 ## R squared (OOB): 0.7319486 rf_predicted \u0026lt;- bind_cols(.fitted = rf_fit1$fit$predictions, data = coff_train) select(rf_predicted, id, cupper_points, .fitted) ## # A tibble: 834 x 3 ## id cupper_points .fitted ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 8.58 8.60 ## 2 3 9.25 8.31 ## 3 5 8.58 8.54 ## 4 10 8.5 8.41 ## 5 12 8.5 8.30 ## 6 13 8.33 8.36 ## 7 16 8.17 8.33 ## 8 19 8.42 8.24 ## 9 21 8.17 8.29 ## 10 23 8.58 8.14 ## # … with 824 more rows ggplot(rf_predicted, aes(.fitted, cupper_points)) + geom_point(alpha = .2) + # ggrepel::geom_label_repel(aes(label = id), # data = filter(rf_predicted, abs(.resid) \u0026gt; 2)) + labs(title = \u0026quot;Random Forest: Actual vs. Predicted Cupper Points\u0026quot;) + geom_smooth() results_train \u0026lt;- lm_fit1 %\u0026gt;% predict(new_data = coff_train) %\u0026gt;% mutate( truth = coff_train$cupper_points, model = \u0026quot;lm\u0026quot; ) %\u0026gt;% bind_rows(rf_fit1 %\u0026gt;% predict(new_data = coff_train) %\u0026gt;% mutate( truth = coff_train$cupper_points, model = \u0026quot;rf\u0026quot; )) results_train %\u0026gt;% group_by(model) %\u0026gt;% rmse(truth = truth, estimate = .pred) ## # A tibble: 2 x 4 ## model .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 lm rmse standard 0.198 ## 2 rf rmse standard 0.114 Apply model to the test set\nlm_fit1 %\u0026gt;% predict(coff_test) %\u0026gt;% bind_cols(coff_test) %\u0026gt;% metrics(truth = cupper_points, estimate = .pred) %\u0026gt;% bind_cols(model = \u0026quot;lm\u0026quot;) %\u0026gt;% bind_rows( rf_fit1 %\u0026gt;% predict(coff_test) %\u0026gt;% bind_cols(coff_test) %\u0026gt;% metrics(truth = cupper_points, estimate = .pred ) %\u0026gt;% bind_cols(model = \u0026quot;rf\u0026quot;) ) ## # A tibble: 6 x 4 ## .metric .estimator .estimate model ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 rmse standard 0.244 lm ## 2 rsq standard 0.629 lm ## 3 mae standard 0.144 lm ## 4 rmse standard 0.231 rf ## 5 rsq standard 0.664 rf ## 6 mae standard 0.130 rf We see that the random forest and linear model are both very close together in performance. However, the random forest does have a small RMSE and higher R squared, which means it does a slightly better job explaining the variance and has less overall error in predicting the ratings.\n Next Steps I did all of this analysis without knowing anything about tidymodels. Additionally, I am fairly new to using R for predicting values as well. It would be interesting to see how these models perform with cross-validation, optimized parameters and better feature selection.\nThe concept of a “recipe” which you can modify and reuse is incredibly interesting in this context. I have a lot more to learn here, and would direct readers to the great YouTube videos made by Julia Silge.\n ",
    "ref": "/blog/coffee-ratings/"
  },{
    "title": "Markdown Syntax Guide",
    "date": "",
    "description": "Sample article showcasing basic Markdown syntax and formatting for HTML elements.",
    "body": "This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item  First Sub-item Second Sub-item    Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ",
    "ref": "/blog/markdown-syntax/"
  },{
    "title": "Rich Content",
    "date": "",
    "description": "A brief description of Hugo Shortcodes",
    "body": "Hugo ships with several Built-in Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\n Instagram Simple Shortcode .__h_instagram.card { font-family: -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; font-size: 14px; border: 1px solid rgb(219, 219, 219); padding: 0; margin-top: 30px; } .__h_instagram.card .card-header, .__h_instagram.card .card-body { padding: 10px 10px 10px; } .__h_instagram.card img { width: 100%; height: auto; }  koloot.design   View More on Instagram    YouTube Privacy Enhanced Shortcode    Twitter Simple Shortcode .twitter-tweet { font: 14px/1.45 -apple-system,BlinkMacSystemFont,\"Segoe UI\",Roboto,Oxygen-Sans,Ubuntu,Cantarell,\"Helvetica Neue\",sans-serif; border-left: 4px solid #2b7bb9; padding-left: 1.5em; color: #555; } .twitter-tweet a { color: #2b7bb9; text-decoration: none; } blockquote.twitter-tweet a:hover, blockquote.twitter-tweet a:focus { text-decoration: underline; }  “In addition to being more logical, asymmetry has the advantage that its complete appearance is far more optically effective than symmetry.”\n— Jan Tschichold pic.twitter.com/gcv7SrhvJb\n\u0026mdash; Graphic Design History (@DesignReviewed) January 17, 2019  Vimeo Simple Shortcode  .__h_video { position: relative; padding-bottom: 56.23%; height: 0; overflow: hidden; width: 100%; background: #000; } .__h_video img { width: 100%; height: auto; color: #000; } .__h_video .play { height: 72px; width: 72px; left: 50%; top: 50%; margin-left: -36px; margin-top: -36px; position: absolute; cursor: pointer; }  ",
    "ref": "/blog/rich-content/"
  },{
    "title": "Placeholder Text",
    "date": "",
    "description": "Lorem Ipsum Dolor Si Amet",
    "body": "Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n",
    "ref": "/blog/placeholder-text/"
  },{
    "title": "Emoji Support",
    "date": "",
    "description": "Guide to emoji usage in Hugo",
    "body": "Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site’s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 🙈 🙉 🙉 🙊 🙊\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n N.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n.emoji { font-family: Apple Color Emoji,Segoe UI Emoji,NotoColorEmoji,Segoe UI Symbol,Android Emoji,EmojiSymbols; }",
    "ref": "/blog/emoji-support/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]
